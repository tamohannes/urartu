# @package _global_
action_name: run_circuit
debug: false

action_config:
  experiment_name: "discogp with blimp"
  device: "cuda" # auto, cuda, cpu (default) 

  task:
    model:
      type:
        _target_: urartu.models.disco_gp.circuit_lm.CircuitTransformer
      name: "gpt2"
      dtype: torch.bfloat16
      generate:
        max_length: 100
        num_beams: 5
        no_repeat_ngram_size: 2

    dataset:
      task_type: 'blimp'
      task: 'anaphor_number_agreement'
      batch_size: 32
      train_test_split: 0.3
      shuffle: True

  
  optuna:
    use_optuna: True
    number_of_trial: 20
    weight_lr: [0.4, 1]
    weight_lambda_sparse_init: [0, 1]
    weight_lambda_complete_init: [1, 10]
    edge_lr: [0.03, 0.5]
    edge_lambda_sparse_init: [0, 1]
    edge_lambda_complete_init: [5, 20]

  weight_hparams:
    use_weight_masks: True
    gs_temp_weight: 0.01
    logits_w_init: 1.0
    lr: 0.5376557319099762
    lambda_sparse_init: 0.7312822389962907 #[0 -> 1]
    lambda_complete_init: 4.328231813945975 # [1 -> 10]
    min_times_lambda_sparse: 1
    max_times_lambda_sparse: 1000

    train_epochs: 300
    n_epoch_warmup_lambda_sparse: 500
    n_epoch_cooldown_lambda_sparse: 1

  edge_hparams:
    use_edge_masks: True
    gs_temp_edge: 0.01
    logits_e_init: 1.0
    lr: 0.474370890393508 #0.1
    lambda_sparse_init: 0.3801449932063512
    lambda_complete_init: 19.97181564013833
    min_times_lambda_sparse: 1.
    max_times_lambda_sparse: 1000.
    train_epochs: 100
    n_epoch_warmup_lambda_sparse: 500
    n_epoch_cooldown_lambda_sparse: 1

  exp_cfg:
    evaluate_every: 1

